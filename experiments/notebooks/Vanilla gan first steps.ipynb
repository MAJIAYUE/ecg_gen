{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  key concept - sygnal normalization\n",
    "#         curr_lead_data = data_dict[patient][lead_n, :]/np.abs(data_dict[patient][lead_n, :]).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence, PackedSequence\n",
    "# import torch.autograd as autograd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import random\n",
    "import tqdm\n",
    "import os\n",
    "import pickle\n",
    "# mnist = input_data.read_data_sets('../../MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "unn_data_dict = pickle.load(open(\"./unn_data.pickle\",'rb'))\n",
    "icbeb_data_dict = pickle.load(open(\"./icbeb_data.pickle\",'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 0:\n",
    "    torch.cuda.manual_seed_all(123)\n",
    "torch.manual_seed(123)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECG_Dataset(Dataset):\n",
    "    def __init__(self, patients_dict: dict, labels_filepath: str, transform_strategy='cut'):\n",
    "        self.transform_strategy = transform_strategy\n",
    "        self.dataset = self.get_pairset(patients_dict, labels_filepath)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.dataset[idx]\n",
    "        if self.transform_strategy:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "    \n",
    "    def get_pairset(self, patients_dict: dict, labels_filepath: str):\n",
    "        labels_df = pd.read_csv(labels_filepath, header=0).set_index('patient')\n",
    "        return [(torch.Tensor(patients_dict[f\"{patient}\"].T),\n",
    "                 torch.Tensor(labels_df.loc[patient].values.astype(bool).astype(int).astype(float))) \n",
    "                for patient in labels_df.index]\n",
    "    def transform(self, sample):\n",
    "        # TODO 1st make cut transform_strategy\n",
    "        return sample\n",
    "    \n",
    "    def parse_labels_file(self, labels_filepath):\n",
    "        return pd.read_csv(labels_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "unn_dataset = ECG_Dataset(unn_data_dict, \"./unn_labels.csv\")\n",
    "icbeb_dataset = ECG_Dataset(icbeb_data_dict, \"./icbeb_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, labels_dim, hidden_dim, latent_dim, \n",
    "                 device='cpu',\n",
    "                 decoder_output_dim=12, \n",
    "                 num_layers=2, \n",
    "                 dropout=0.2, \n",
    "                 batch_first=True,\n",
    "                 bidirectional=False,\n",
    "                 batch_size=1,\n",
    "                 unique_diagnosis_labels=[torch.from_numpy(np.array(_i, dtype=np.float32)) \n",
    "                           for _i in set([tuple(it[1].numpy().tolist()) for it in icbeb_dataset + unn_dataset])],\n",
    "                 sample_len_collection=[5000, 7500, 10000, 12500, 15000]\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Create a Generator object for time-series generation\n",
    "        \"\"\"\n",
    "        super(Generator, self).__init__()\n",
    "        self.labels_dim = labels_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.device = device\n",
    "        self.decoder_output_dim = decoder_output_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.batch_first = batch_first\n",
    "        self.bidirectional= bidirectional\n",
    "        self.batch_size = batch_size\n",
    "        self.unique_diagnosis_labels = unique_diagnosis_labels\n",
    "        self.sample_len_collection = sample_len_collection\n",
    "        self.lstm_output_mult = 1 + int(self.bidirectional)\n",
    "        # Define our lstm layer\n",
    "        self.lstm = nn.LSTM(input_size=self.labels_dim + self.latent_dim,\n",
    "                            hidden_size=self.hidden_dim,\n",
    "                            num_layers=self.num_layers,\n",
    "                            batch_first=self.batch_first,\n",
    "                            dropout=self.dropout,\n",
    "                            bidirectional=self.bidirectional,\n",
    "                           ).to(self.device)\n",
    "        # Define our decoder layer\n",
    "        self.decoder = nn.Linear(self.hidden_dim * self.lstm_output_mult, self.decoder_output_dim).to(self.device)\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        return tuple([torch.zeros((self.num_layers, self.batch_size, self.hidden_dim), \n",
    "                                  dtype=torch.float32).to(self.device)\n",
    "                      for _ in range(self.num_layers)])\n",
    "    \n",
    "    def forward(self, _input, _hidden):\n",
    "        \"\"\"\n",
    "            if _category == None then _input already combined\n",
    "        \"\"\"\n",
    "        input_combined = _input.to(self.device)\n",
    "        \n",
    "        raw_output, _hidden = self.lstm(input_combined, _hidden)\n",
    "        if isinstance(raw_output, PackedSequence):\n",
    "            raw_output, lengths = pad_packed_sequence(raw_output, batch_first=self.batch_first)\n",
    "        decoded_output = self.decoder(raw_output)\n",
    "        return decoded_output, _hidden\n",
    "    \n",
    "    def get_nllt_list(self, size):\n",
    "        return [(torch.rand((1, self.latent_dim), dtype=torch.float32).expand(_len, -1).to(self.device), \n",
    "                 random.choice(self.unique_diagnosis_labels).unsqueeze(0).float().expand(_len, -1).to(self.device),\n",
    "                 _len)\n",
    "                for _len in random.choices(self.sample_len_collection, k=size)]\n",
    "    \n",
    "    def generate_seq_batch(self):\n",
    "        noise_label_len_tuple_list = self.get_nllt_list(self.batch_size)\n",
    "        sorted_nllt_list = sorted(noise_label_len_tuple_list, key=lambda x: x[2], reverse=True)\n",
    "\n",
    "        noises = [x[0] for x in sorted_nllt_list]\n",
    "        labels = [x[1] for x in sorted_nllt_list]\n",
    "        lenghts = [x[2] for x in sorted_nllt_list]\n",
    "        \n",
    "        noises_padded = pad_sequence(noises, batch_first=self.batch_first)\n",
    "        labels_padded = pad_sequence(labels, batch_first=self.batch_first)\n",
    "#         print(f\"{noises_padded.shape}, {labels_padded.shape}\")\n",
    "        labels_noises_padded = torch.cat([labels_padded, noises_padded], 2)\n",
    "        \n",
    "        labels_noises_packed = pack_padded_sequence(labels_noises_padded, lenghts, batch_first=self.batch_first)\n",
    "        \n",
    "        generated_seq, _ = self.forward(labels_noises_packed, self.init_hidden())\n",
    "        \n",
    "        return generated_seq, lenghts, labels_padded\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_ecg_example(seq_batch[0].detach().numpy(), 'test_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim,\n",
    "                 labels_dim, \n",
    "                 hidden_dim,\n",
    "                 encoder_dim,\n",
    "                 decoder_dim,\n",
    "                 batch_size, \n",
    "                 device='cpu',\n",
    "                 num_layers=2, \n",
    "                 dropout=0.2, \n",
    "                 batch_first=True,\n",
    "                 bidirectional=False):\n",
    "        \"\"\"\n",
    "        Create a Discriminator object for time-series clasification -- real or fake\n",
    "        \"\"\"\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.labels_dim = labels_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.encoder_dim = encoder_dim\n",
    "        self.decoder_dim = decoder_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.bidirectional= bidirectional\n",
    "        self.batch_first = batch_first\n",
    "        self.lstm_output_mult = 1 + int(self.bidirectional)\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "                                    nn.Linear(self.labels_dim + self.input_dim, self.encoder_dim),\n",
    "                                    nn.ReLU()\n",
    "                                    ).to(self.device)\n",
    "        # Define our lstm layer\n",
    "        self.lstm = nn.LSTM(input_size=self.encoder_dim,\n",
    "                            hidden_size=self.hidden_dim,\n",
    "                            num_layers=self.num_layers,\n",
    "                            batch_first=self.batch_first,\n",
    "                            dropout=self.dropout,\n",
    "                            bidirectional=self.bidirectional,\n",
    "                           ).to(self.device)\n",
    "        # Define our decoder layer\n",
    "        self.decoder = nn.Sequential(\n",
    "                                    nn.Linear(self.hidden_dim * self.lstm_output_mult, self.decoder_dim),\n",
    "                                    nn.Sigmoid()\n",
    "                                    ).to(self.device)\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        return tuple([torch.zeros((self.num_layers, self.batch_size, self.hidden_dim), \n",
    "                                  dtype=torch.float32).to(self.device) \n",
    "                      for _ in range(self.num_layers)])\n",
    "    \n",
    "    def forward(self, _input, _hidden=None):\n",
    "        \"\"\"\n",
    "            if _category == None then _input already combined\n",
    "        \"\"\"\n",
    "        if _hidden is None:\n",
    "            _hidden = self.init_hidden()\n",
    "            \n",
    "#         if _category:\n",
    "#             input_combined = torch.cat((_category, _input), 2)\n",
    "#         else:\n",
    "#             input_combined = _input\n",
    "\n",
    "        encoder_input, lengths = pad_packed_sequence(_input, batch_first=self.batch_first)\n",
    "\n",
    "        encoded_output = self.encoder(encoder_input)\n",
    "\n",
    "        packed_encoded_output = pack_padded_sequence(encoded_output, lengths, batch_first=self.batch_first)\n",
    "\n",
    "        rnn_output, _hidden = self.lstm(packed_encoded_output, _hidden)\n",
    "        if isinstance(rnn_output, PackedSequence):\n",
    "            rnn_output, lengths = pad_packed_sequence(rnn_output, batch_first=self.batch_first)\n",
    "\n",
    "        decoded_output = self.decoder(rnn_output)\n",
    "        return decoded_output, lengths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_ecg_example(gen_data: np.array, epoch_number, _title='12-lead ECG'):\n",
    "    fig = plt.figure(figsize=(12,14),)\n",
    "    for lead_n in range(gen_data.shape[1]):\n",
    "    #             key concept - sygnal normalization\n",
    "        curr_lead_data = gen_data[:, lead_n]\n",
    "        ax = plt.subplot(4, 3, lead_n+1)\n",
    "        plt.plot(curr_lead_data, label=f'lead_{lead_n+1}')\n",
    "        plt.title(f'lead_{lead_n+1}')\n",
    "    fig.suptitle(_title)\n",
    "    plt.savefig(f'out/{epoch_number}.png', bbox_inches='tight')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_batch_sequence(batch):\n",
    "    sorted_batch = sorted(batch, key=lambda x: x[0].shape[0], reverse=True)\n",
    "    sequences = [x[0] for x in sorted_batch]\n",
    "    sequences_padded = pad_sequence(sequences, batch_first=True)\n",
    "    lengths = [len(x) for x in sequences]\n",
    "    # Don't forget to grab the labels of the *sorted* batch\n",
    "    labels = [x[1].unsqueeze(0).expand(len(x[0]), -1) for x in sorted_batch]\n",
    "    labels_padded = pad_sequence(labels, batch_first=True)\n",
    "    return sequences_padded, lengths, labels_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')\n",
    "# device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for training\n",
    "## general params\n",
    "n_epochs = 10\n",
    "batch_size = 24\n",
    "lr = 1e-3\n",
    "labels_dim = 7\n",
    "lead_n = 12\n",
    "## generator params\n",
    "gen_h_dim = 128\n",
    "gen_l_dim = 100\n",
    "## discriminator params\n",
    "dis_h_dim = 128\n",
    "dis_encoder_h_dim = 128\n",
    "# loss and dataloader setup\n",
    "criterion = nn.BCELoss()\n",
    "real_data_loader = DataLoader(icbeb_dataset, batch_size=batch_size, \n",
    "                              collate_fn=pad_batch_sequence, shuffle=True,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = Generator(labels_dim=labels_dim, \n",
    "              hidden_dim=gen_h_dim, \n",
    "              latent_dim=gen_l_dim, \n",
    "              batch_size=batch_size, \n",
    "              device=device, \n",
    "              decoder_output_dim=lead_n)\n",
    "D = Discriminator(input_dim=lead_n, \n",
    "                  labels_dim=labels_dim, \n",
    "                  hidden_dim=dis_h_dim, \n",
    "                  encoder_dim=dis_encoder_h_dim, \n",
    "                  decoder_dim=1, \n",
    "                  batch_size=batch_size, \n",
    "                  device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not os.path.exists('out/'):\n",
    "os.makedirs('out/pictures', exist_ok=True)\n",
    "os.makedirs('out/models', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==1.1.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/60/f685fb2cfb3088736bafbc9bdbb455327bdc8906b606da9c9a81bae1c81e/torch-1.1.0-cp36-cp36m-manylinux1_x86_64.whl (676.9MB)\n",
      "\u001b[K    100% |████████████████████████████████| 676.9MB 84kB/s eta 0:00:011   11% |███▊                            | 79.3MB 2.1MB/s eta 0:04:40    45% |██████████████▋                 | 308.2MB 3.1MB/s eta 0:01:59    48% |███████████████▋                | 330.4MB 1.8MB/s eta 0:03:18    62% |████████████████████            | 423.9MB 5.1MB/s eta 0:00:50    65% |█████████████████████           | 446.1MB 1.2MB/s eta 0:03:13    69% |██████████████████████▎         | 471.0MB 2.5MB/s eta 0:01:22    70% |██████████████████████▌         | 475.6MB 1.4MB/s eta 0:02:26    70% |██████████████████████▊         | 480.2MB 1.5MB/s eta 0:02:09    71% |██████████████████████▉         | 482.4MB 1.4MB/s eta 0:02:21    75% |████████████████████████▏       | 512.2MB 984kB/s eta 0:02:48    77% |████████████████████████▉       | 524.7MB 813kB/s eta 0:03:08    79% |█████████████████████████▌      | 538.8MB 2.5MB/s eta 0:00:55    82% |██████████████████████████▌     | 561.5MB 1.2MB/s eta 0:01:34    86% |███████████████████████████▉    | 587.7MB 1.1MB/s eta 0:01:18�██████████████▌| 665.9MB 34.3MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /out_world/university/diploma_env/lib/python3.6/site-packages (from torch==1.1.0) (1.16.2)\n",
      "Installing collected packages: torch\n",
      "  Found existing installation: torch 1.0.1.post2\n",
      "    Uninstalling torch-1.0.1.post2:\n",
      "      Successfully uninstalled torch-1.0.1.post2\n",
      "Successfully installed torch-1.1.0\n",
      "\u001b[33mYou are using pip version 19.0.3, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install torch==1.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecccc786cbf648ddb55faf182e2acfc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f924eee55c7944f7aa231a27bf5c005c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-0; D_loss: 1.4149549007415771; G_loss: 0.6764982342720032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/out_world/university/diploma_env/lib/python3.6/site-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type Discriminator. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/out_world/university/diploma_env/lib/python3.6/site-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type Generator. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9b56a2e84fd4846afe42158bf1195a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "763fb5fbe246484c9f0427a6254d2be8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-2; D_loss: 1.418543815612793; G_loss: 0.5454565286636353\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6c2360ec5a247eb84de37772e399a4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-b12a76a883db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0md_fake_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_fake_predictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# ... and real ones\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0md_real_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_pred_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_packed_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0md_real_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_real_predictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# Now we can calculate loss for discriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/out_world/university/diploma_env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-10b11c732bdd>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, _input, _hidden)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mpacked_encoded_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_first\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mrnn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked_encoded_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mrnn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_packed_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_first\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/out_world/university/diploma_env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/out_world/university/diploma_env/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             result = _impl(input, batch_sizes, hx, self._flat_weights, self.bias,\n\u001b[0;32m--> 182\u001b[0;31m                            self.num_layers, self.dropout, self.training, self.bidirectional)\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'LSTM'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\" ===================== TRAINING ======================== \"\"\"\n",
    "\n",
    "\n",
    "G_optimizer = optim.Adam(G.parameters(), lr=lr)\n",
    "D_optimizer = optim.Adam(D.parameters(), lr=lr)\n",
    "\n",
    "# ones_label = torch.ones(mb_size, 1)\n",
    "# zeros_label = torch.zeros(mb_size, 1)\n",
    "\n",
    "writer = SummaryWriter()\n",
    "for epoch in tqdm.tqdm_notebook(range(n_epochs),position=0):\n",
    "    # sequences in true_data_loader already padded thanks to pad_batch_sequence function\n",
    "    for real_seqs, real_lenghts, real_labels in tqdm.tqdm_notebook([it for it in real_data_loader][:5], position=1):\n",
    "        \n",
    "# ------------------------------ Discriminator step --------------------------------------\n",
    "        # Generate fake sample,\n",
    "        fake_seq_padded_batch, fake_lengths, fake_labels_padded_batch = G.generate_seq_batch()\n",
    "        # Let's prepare our fake and real samples \n",
    "        # concat labels + sequence alongside\n",
    "        fake_label_seq_padded_batch = torch.cat([fake_labels_padded_batch, fake_seq_padded_batch], 2)\n",
    "        fake_packed_batch = pack_padded_sequence(fake_label_seq_padded_batch, fake_lengths, batch_first=True)\n",
    "        # and the same for real ones\n",
    "        real_label_seq_padded_batch = torch.cat([real_labels, real_seqs], 2)\n",
    "        real_packed_batch = pack_padded_sequence(real_label_seq_padded_batch, real_lenghts, batch_first=True)\n",
    "        # After that we can make predictions for our fake examples\n",
    "        d_fake_predictions, fake_pred_lengths = D(fake_packed_batch)\n",
    "        d_fake_target = torch.zeros_like(d_fake_predictions)\n",
    "        # ... and real ones\n",
    "        d_real_predictions, real_pred_lengths = D(real_packed_batch.to(device))\n",
    "        d_real_target = torch.ones_like(d_real_predictions)\n",
    "        # Now we can calculate loss for discriminator\n",
    "        # TODO Before calc loss we need to make sure that sequence with vary length is ok for that \n",
    "        d_fake_loss = criterion(d_fake_predictions, d_fake_target)\n",
    "        d_real_loss = criterion(d_real_predictions, d_real_target)\n",
    "        d_loss = d_real_loss + d_fake_loss\n",
    "        # And make backpropagation according to calculated loss\n",
    "        d_loss.backward()\n",
    "        D_optimizer.step()\n",
    "        # Housekeeping - reset gradient\n",
    "        D_optimizer.zero_grad()\n",
    "        \n",
    "# ---------------------------- Generator step ----------------------------------------------\n",
    "        # Generate fake sample\n",
    "        fake_seq_padded_batch, fake_lengths, fake_labels_padded_batch = G.generate_seq_batch()\n",
    "        # concat labels + sequence alongside\n",
    "        fake_label_seq_padded_batch = torch.cat([fake_labels_padded_batch, fake_seq_padded_batch], 2)\n",
    "        fake_packed_batch = pack_padded_sequence(fake_label_seq_padded_batch, fake_lengths, batch_first=True)\n",
    "        # After that we can make predictions for our fake examples\n",
    "        d_fake_predictions, fake_pred_lengths = D(fake_packed_batch)\n",
    "        g_target = torch.ones_like(d_fake_predictions)\n",
    "        # Now we can calculate loss for generator\n",
    "        g_loss = criterion(d_fake_predictions, g_target)\n",
    "        # And make backpropagation according to calculated loss\n",
    "        g_loss.backward()\n",
    "        G_optimizer.step()\n",
    "        # Housekeeping - reset gradient\n",
    "        G_optimizer.zero_grad()\n",
    "    # plot example each 100 epochs\n",
    "    if epoch % 2 == 0:\n",
    "        print(f'Epoch-{epoch}; D_loss: {d_loss.data.cpu().numpy()}; G_loss: {g_loss.data.cpu().numpy()}')\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            \"d_model\": D,\n",
    "            \"d_loss\": d_loss,\n",
    "            \"d_optimizer\": D_optimizer,\n",
    "            \"g_model\": G,\n",
    "            \"g_loss\": g_loss,\n",
    "            \"g_optimizer\": G_optimizer,\n",
    "            }, f\"./out/models/epoch_{epoch}_checkpoint.pkl\")        \n",
    "        with torch.no_grad():\n",
    "            pass\n",
    "            \n",
    "#         feasible_label = np.zeros(shape=[mb_size, y_dim], dtype='float32')\n",
    "#         feasible_label[:, np.random.randint(0, 10)] = 1.\n",
    "#         feasible_label = Variable(torch.from_numpy(feasible_label))\n",
    "#         samples = G(z, feasible_label).data.numpy()[:16]\n",
    "        \n",
    "        # TODO take visualize func here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier\n",
    "- normalise unn signal and overwrite related pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
